{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"url_empresas\")\n",
    "\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"ETL_Empresas\")\n",
    "        .config(\"spark.jars\", os.getenv(\"bigquery_jar\"))\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .config(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7a99de",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_STR_OR_LIST_OF_RDD] Argument `path` should be a str or list[RDD], got Response.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m resposta \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url_empresas, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m resposta\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m----> 6\u001b[0m arquivo_csv \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresposta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([StructField(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_c\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)])\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m (spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquote\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     12\u001b[0m                 \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mescape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;241m.\u001b[39mcsv(arquivo_csv)\n\u001b[0;32m     18\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:763\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(jdataset))\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    764\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR_OR_LIST_OF_RDD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    765\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    766\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    767\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    768\u001b[0m         },\n\u001b[0;32m    769\u001b[0m     )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_STR_OR_LIST_OF_RDD] Argument `path` should be a str or list[RDD], got Response."
     ]
    }
   ],
   "source": [
    "url_empresas = \"https://engenheiro-dados-dados-teste.s3.sa-east-1.amazonaws.com/empresas.csv\"\n",
    "\n",
    "logging.info(f\"--> Baixando arquivo de: {url_empresas}\")\n",
    "resposta = requests.get(url_empresas, stream=True)\n",
    "resposta.raise_for_status()\n",
    "arquivo_csv = spark.read.csv(resposta)\n",
    "\n",
    "schema = StructType([StructField(f\"_c{i}\", StringType(), True) for i in range(30)])\n",
    "\n",
    "df = (spark.read.option(\"sep\", \";\")\n",
    "                .option(\"quote\", '\"') \n",
    "                .option(\"escape\", '\"')\n",
    "                .option(\"multiLine\", True) \n",
    "                .option(\"header\", \"false\")\n",
    "                .option(\"encoding\", \"latin1\")\n",
    "                .schema(schema)\n",
    "                .csv(arquivo_csv)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadce724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(arquivo_parquet)\n",
    "print(tabulate(df_municipios.head(10), headers='keys', tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0489ab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_APPLICATION_CREDENTIALS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAmim Santana\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgcp-keys\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata-engineer-project-25-51defe1513a2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m client \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mClient()\n\u001b[1;32m---> 22\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_empresas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m job\u001b[38;5;241m.\u001b[39mresult()  \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Configuração do logs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:2864\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[1;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[0;32m   2846\u001b[0m         columns_and_indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[0;32m   2847\u001b[0m             name\n\u001b[0;32m   2848\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m _pandas_helpers\u001b[38;5;241m.\u001b[39mlist_columns_and_indexes(dataframe)\n\u001b[0;32m   2849\u001b[0m         )\n\u001b[0;32m   2850\u001b[0m         new_job_config\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2851\u001b[0m             \u001b[38;5;66;03m# Field description and policy tags are not needed to\u001b[39;00m\n\u001b[0;32m   2852\u001b[0m             \u001b[38;5;66;03m# serialize a data frame.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2861\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m columns_and_indexes\n\u001b[0;32m   2862\u001b[0m         ]\n\u001b[1;32m-> 2864\u001b[0m new_job_config\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m \u001b[43m_pandas_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe_to_bq_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_job_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\n\u001b[0;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_job_config\u001b[38;5;241m.\u001b[39mschema:\n\u001b[0;32m   2869\u001b[0m     \u001b[38;5;66;03m# the schema could not be fully detected\u001b[39;00m\n\u001b[0;32m   2870\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema could not be detected for all columns. Loading from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe without a schema will be deprecated in the future, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2875\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   2876\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:491\u001b[0m, in \u001b[0;36mdataframe_to_bq_schema\u001b[1;34m(dataframe, bq_schema)\u001b[0m\n\u001b[0;32m    484\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pandas DataFrame into BigQuery will require pandas-gbq \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage version 0.26.1 or greater in the future. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to import pandas-gbq and got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_gbq_import_exception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    488\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_gbq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_to_bigquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe_to_bigquery_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_bigquery_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbq_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bq_schema:\n\u001b[0;32m    498\u001b[0m     bq_schema \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39m_to_schema_fields(bq_schema)\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas_gbq\\schema\\pandas_to_bigquery.py:112\u001b[0m, in \u001b[0;36mdataframe_to_bigquery_fields\u001b[1;34m(dataframe, override_bigquery_fields, default_type, index)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Try to automatically determine the type based on a few rows of the data.\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[column]\n\u001b[0;32m    113\u001b[0m bq_field \u001b[38;5;241m=\u001b[39m values_to_bigquery_field(column, values, default_type\u001b[38;5;241m=\u001b[39mdefault_type)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bq_field:\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:6306\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[0;32m   6304\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   6305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6306\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_duplicates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   6308\u001b[0m     allow_duplicates \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(allow_duplicates, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6368\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6258\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6261\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6366\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6368\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:649\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    647\u001b[0m     new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 649\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m new_refs: \u001b[38;5;28mlist\u001b[39m[weakref\u001b[38;5;241m.\u001b[39mref \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\Amim Santana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:550\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m    549\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m(values, placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr_locs, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bibliotecas \n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from pandas_gbq import to_gbq\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# Configuração do logs\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# URLs\n",
    "url_empresas = \"https://engenheiro-dados-dados-teste.s3.sa-east-1.amazonaws.com/empresas.csv\"\n",
    "# url_municipios = \"\"\n",
    "\n",
    "# Funções do pipeline\n",
    "def carregar_dados():\n",
    "    logging.info(\"Carregando dados das empresas...\")\n",
    "    inicio_carga = time.time()\n",
    "\n",
    "    df_empresas = pd.read_csv(url_empresas, sep=';', encoding='latin1', dtype=str, header=None)\n",
    "    fim_carga = time.time()\n",
    "\n",
    "    tempo_execucao = fim_carga - inicio_carga\n",
    "    logging.info(f\"Carregamento concluido! Tempo execução: {tempo_execucao:.2f} segundos.\")\n",
    "\n",
    "    return df_empresas\n",
    "\n",
    "def tratamento_dados(df_empresas):\n",
    "    logging.info(\"Iniciando o tratamento dos dados...\")\n",
    "    inicio_tratamento = time.time()\n",
    "\n",
    "    # Renomeando as colunas\n",
    "    df_empresas.columns = [\n",
    "        \"basico\", \"ordem\", \"dv\", \"identificador\", \"empresa\", \"sit_cadastral\",\n",
    "        \"dt_sit_cadastral\", \"mot_sit_cadastral\", \"cidade_ext\", \"pais\", \"dt_inicio\",\n",
    "        \"cnae_principal\", \"cnae_secundario\", \"tp_logradouro\", \"logradouro\", \"numero\",\n",
    "        \"complemento\", \"bairro\", \"cep\", \"uf\", \"municipio\", \"ddd_telefone_1\",\n",
    "        \"telefone_1\", \"ddd_telefone_2\", \"telefone_2\", \"ddd_fax\", \"telefone_fax\",\n",
    "        \"email\", \"sit_especial\", \"dt_sit_especial\"\n",
    "    ]\n",
    "\n",
    "    # Formatar coluna CNPJ\n",
    "    df_empresas[\"cnpj\"] = (\n",
    "        df_empresas[\"basico\"].str.zfill(8) +\n",
    "        df_empresas[\"ordem\"].str.zfill(4) +\n",
    "        df_empresas[\"dv\"].str.zfill(2)\n",
    "    )\n",
    "    df_empresas[\"cnpj\"] = df_empresas[\"cnpj\"].apply(\n",
    "        lambda cnpj: f\"{cnpj[:2]}.{cnpj[2:5]}.{cnpj[5:8]}/{cnpj[8:12]}-{cnpj[12:]}\"\n",
    "    )\n",
    "\n",
    "    df_empresas.drop([\"basico\", \"ordem\", \"dv\"], axis=1, inplace=True)\n",
    "\n",
    "    # Colunas em string\n",
    "    colunas_texto = [\n",
    "        \"empresa\", \"email\", \"complemento\", \"tp_logradouro\", \"logradouro\",\n",
    "        \"bairro\", \"cnae_principal\", \"cnae_secundario\", \"telefone_1\", \"telefone_2\",\n",
    "        \"telefone_fax\", \"ddd_telefone_1\", \"ddd_telefone_2\", \"ddd_fax\"\n",
    "    ]\n",
    "    for col in colunas_texto:\n",
    "        if col in df_empresas.columns:\n",
    "            df_empresas[col] = df_empresas[col].fillna(\"não informado\").str.lower().str.strip()\n",
    "\n",
    "    # Tratar colunas de data\n",
    "    colunas_datas = [\"dt_sit_cadastral\", \"dt_inicio\", \"dt_sit_especial\"]\n",
    "    for col in colunas_datas:\n",
    "        if col in df_empresas.columns:\n",
    "            df_empresas[col] = pd.to_datetime(df_empresas[col], errors=\"coerce\").dt.date\n",
    "    \n",
    "    # Tratar colunas Matriz/Filial\n",
    "    df_empresas[\"identificador\"] = df_empresas[\"identificador\"].replace({\n",
    "        \"1\": \"matriz\",\n",
    "        \"2\": \"filial\"\n",
    "    })\n",
    "    \n",
    "    # Tratar situação cadastrar\n",
    "    df_empresas[\"sit_cadastral\"] = df_empresas[\"sit_cadastral\"].replace({\n",
    "        \"01\": \"nula\",\n",
    "        \"02\": \"ativa\",\n",
    "        \"03\": \"suspensa\",\n",
    "        \"04\": \"inapta\",\n",
    "        \"08\": \"baixada\"\n",
    "    })\n",
    "\n",
    "    # Tratar numero\n",
    "    df_empresas[\"numero\"] = df_empresas[\"numero\"].fillna(\"s/n\")\n",
    "    \n",
    "    # Convetendo colunas NaN\n",
    "    df_empresas = df_empresas.where(pd.notnull(df_empresas), None)\n",
    "    \n",
    "    fim_tratamento = time.time()\n",
    "    tempo_execucao = fim_tratamento - inicio_tratamento\n",
    "    logging.info(f\"Colunas tratadas! Tempo execução: {tempo_execucao:.2f} segundos.\")\n",
    "\n",
    "    return df_empresas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_empresas = carregar_dados()\n",
    "    df_empresas = tratamento_dados(df_empresas)\n",
    "    print(tabulate(df_empresas.head(20), headers='keys', tablefmt=\"psql\"))\n",
    "    # print(df_empresas_tratado.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
